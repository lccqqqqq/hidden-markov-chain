model:
  name: "MaskedHeadTransformerModel"
  vocab_size: 2
  n_ctx: 10
  n_positions: 10
  d_head: 8
  n_head: 1
  n_layer: 4
  n_embd: 64
  n_inner: 256
  activation_function: "relu"

train:
  process: z1r
  batch_size: 64
  learning_rate: 0.00005
  num_epochs: 24000
  device: "mps"
  wandb_project_name: "hidden-markov-model-test"
  wandb_run_name: "model-cos_scheduler-z1r-lr-0.00005-batch-64"

scheduler:
  type: "cosine"  # Options: "cosine", "step", "exponential", "plateau", "warmup_cosine", "none"
  min_lr: 0.000001  # For cosine annealing
  warmup_epochs: 1000  # For warmup_cosine
  step_size: 5000  # For step scheduler
  gamma: 0.5  # For step/exponential schedulers
  patience: 1000  # For plateau scheduler
  factor: 0.5  # For plateau scheduler

# Dataset configuration (optional)
# Uncomment to use pre-computed datasets instead of on-the-fly generation
dataset:
  mode: "on_the_fly"  # Options: "precomputed" or "on_the_fly"
  # path: "data/datasets/z1r"  # Path to pre-generated dataset (auto-detected if not specified)

  # Split sizes (used when generating datasets)
  val_samples: 5000
  test_samples: 5000

  # Evaluation settings (used during training with precomputed mode)
  eval_interval: 1000  # Evaluate on validation set every N steps

  # Early stopping settings
  early_stopping:
    enabled: false  # Set to true to enable early stopping
    patience: 5000  # Stop if no improvement for N steps
    min_delta: 0.0001  # Minimum change to count as improvement